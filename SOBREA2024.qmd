---
title: "Fatores"
author: "Luiz Fernando Palin Droubi"
date: last-modified
# format: beamer
format:
  revealjs:
    theme: beige
#    smaller: true
    keep-md: true
    incremental: true
prefer-html: true
---

```{r, eval = FALSE}
set.seed(5)
inclinacao <- rlnorm(n=25, meanlog = 1, sd = .75)
```

```{r, eval = FALSE}
set.seed(1)
area <- rlnorm(n = 25, meanlog = 7, sdlog = .75)
frente <- area/rlnorm(n = 25, meanlog = 3.5, sdlog = .5)
#inclinacao <- rlnorm(n=25, meanlog = 1, sd = .75)
pu <- 25000*area^-.25*frente^.10*inclinacao^-.10*rnorm(n=25, mean = 1, sd = .1)
dados <- data.frame(PU = pu, Area = area, Frente = frente, 
                    Incl = inclinacao)
#dados$Frente[sample(1:25, 5)] <- NA
dados$Frente[c(4, 13, 14, 19, 24)] <- NA
#dados$Inclinacao[sample(1:25, 10)] <- NA
dados$Incl[c(2, 3, 11, 13, 14, 17, 18, 20, 23, 24)] <- NA
dados <- as.data.frame(apply(dados, 2, round, 2))
```

```{r}
library(appraiseR)
library(MASS)

# Data parameters
n <- 30
m <- c(450, 15, 4, 2500)
s <- c(175, 4, 3, 250)
ryx1 <- -0.50; ryx2 <- -0.30; ryx3 <- -.40; rx1x2 <- .90; rx1x3 <- 0; rx2x3 <- 0

sigma <- sqrt(log(s^2/m^2 + 1))
mu <- log(m) - sigma^2/2
rhoyx1 <- log(ryx1*prod(s[c(4, 1)])   / prod(m[c(4, 1)]) + 1)
rhoyx2 <- log(ryx2*prod(s[c(4, 2)])   / prod(m[c(4, 2)]) + 1)
rhoyx3 <- log(ryx3*prod(s[c(4, 3)])   / prod(m[c(4, 3)]) + 1)
rhox1x2 <- log(rx1x2*prod(s[1:2])     / prod(m[1:2]) + 1)
rhox1x3 <- log(rx1x3*prod(s[c(1, 3)]) / prod(m[c(1, 3)]) + 1)
rhox2x3 <- log(rx2x3*prod(s[2:3]) / prod(m[2:3]) + 1)

##                 Area         Frente        Incl.         #PU
Sigma <- matrix(c(sigma[1]^2,    rhox1x2,    rhox1x3,     rhoyx1,  # Area
                     rhox1x2, sigma[2]^2,    rhox2x3,     rhoyx2,  # Frente
                     rhox1x3,    rhox2x3, sigma[3]^2,     rhoyx3,  # Incl.
                      rhoyx1,     rhoyx2,     rhoyx3, sigma[4]^2   # PU
                  ),         # Frente
                ncol = 4, byrow = T)

# Create data
set.seed(1)
dados <- exp(mvrnorm(n=n, mu=  mu, Sigma = Sigma,
                     empirical = T))
colnames(dados) <- c("Area", "Frente", "Incl", "PU")
dados <- apply(dados, 2, round, 2)
dados <- as.data.frame(dados)
```

```{r}
# Impute NA's
set.seed(2)
dados$Incl[sample(1:25, 10)] <- NA
dados$Frente[sample(1:25, 5)] <- NA

# Adjust fit
fit <- lm(log(PU) ~ log(Area) + log(Frente) + log(Incl), data = dados)
AreaCoef <- coef(fit)["log(Area)"]
FrenteCoef <- coef(fit)["log(Frente)"]
```

# Introdução

## Sobre a arte de comunicar {.smaller}

> Comunicação não é o que você fala, é o que o outro entende!

-   O Tratamento Científico trouxe **consistência** para a Engenharia de Avaliações

-   O Tratamento por Fatores, contudo, não deixou de ser utilizado

    -   Acredita-se que, em parte, isso se deve à clareza obtida com este tipo de tratamento

-   A consistência alcançada com o Tratamento Científico deu-se a custa de uma perda na clareza da comunicação

-   É possível conciliar?

    -   @upav2024, @valorem2024Augusto, @droubi2021, @Cerino2020, @trivelloni2005

-   Há outras vantagens, além da clareza, na utilização do tratamento por fatores?

## Cobb-Douglas {.smaller}

-   Função de Cobb-Douglas (forma mais simples):

-   $$Y = a.X^b$$

-   É possível linearizar a função de Cobb-Douglas:

-   $$\ln(Y) = \ln(a) + b.\ln(X)$$

-   O que torna fácil estimar a regressão:

-   $$\ln(Y) = \beta_0 + \beta_1.\ln(X) + \varepsilon$$

-   Uma vez estimado o modelo de regressão acima, pode-se obter $\hat a$ e $\hat b$:

-   $$\hat a = \exp(\hat \beta_0);\, \hat b = \hat\beta_1$$

-   A linearização está centrada na hipótese de que o erro é **multiplicativo**:

-   $\hat \varepsilon = \frac{Y}{\hat aX^\hat b}=\frac{Y}{\hat Y}$

## Cobb-Douglas (2) {.smaller}

-   A hipótese de que o erro é aditivo pode não ser verificada no mercado imobiliário:

. . .

```{r}
#| label: residuosAditivos
#| fig-cap: "Termo de erro aditivo."

library(ggplot2)
library(ggthemes)
library(ggResidpanel)
theme_set(theme_solarized(base_size = 15))
data(zilli_2020)
fitZilli <- lm(PU ~ BRO*log(DABM/100) + PC + I(NG-1) + log(AP/100),
          data = zilli_2020, subset = -204)
#olsrr::ols_plot_resid_stud_fit(fitZilli, threshold = 2.5)
resid_panel(fitZilli)
```

## Cobb-Douglas (3) {.smaller}

```{r}
#| label: residuosMultiplicativos
#| fig-cap: "Termo de erro multiplicativo"
data(zilli_2020)
fitZilli1 <- update(fitZilli, log(PU) ~.)
# olsrr::ols_plot_resid_stud_fit(fitZilli1, threshold = 2.5)
resid_panel(fitZilli1)
```

-   A hipótese dos erros multiplicativos parece mais adequada!

## Cobb-Douglas (4) {.smaller}

```{r}
library(appraiseR)
library(MASS)

# Data parameters
n <- 60
m <- c(450, 7500)
s <- c(250, 1250)
ryx <- -0.70

sigma <- sqrt(log(s^2/m^2 + 1))
mu <- log(m) - sigma^2/2
rhoyx <- log(ryx*prod(s[c(2, 1)])   / prod(m[c(2, 1)]) + 1)

##                 Area             #PU
Sigma <- matrix(c(sigma[1]^2,     rhoyx,  # Area
                      rhoyx, sigma[2]^2   # PU
                  ),    
                ncol = 2, byrow = T)

# Create data
set.seed(2)
df <- exp(mvrnorm(n=n, mu=  mu, Sigma = Sigma,
                     empirical = T))
colnames(df) <- c("Area", "PU")
df <- apply(df, 2, round, 2)
df <- as.data.frame(df)
```

```{r}
#| label: nlsFit
#| fig-cap: "Dados para exemplificar a equação de Cobb-Douglas."
library(ggtext)
library(ggpmisc)
ggplot(df, aes(x = Area, y = PU)) +
  geom_point() +
  stat_smooth(method="nls", se=FALSE, formula=y~a*log(x/450) + k,
              method.args=list(start=c(a=-100, k=10000))) +
  stat_poly_line(color = "red", lty = 2, se = FALSE) +
  labs(title = "<b style='color:red'>Modelo Linear</b> e 
  <b style='color:blue'>Modelo Não-Linear</b>") +
  theme(plot.title = element_markdown(lineheight = 1.1))
```

## Cobb-Douglas (5) {.smaller}

```{r}
ggplot(df, aes(x = Area, y = PU)) +
  geom_point() +
  stat_poly_line() +
  stat_poly_eq(use_label(c("eq", "R2")), label.x = "right", color = "blue",
               size = 7) +
  scale_y_continuous(transform = "log") +
  scale_x_continuous(transform = "log", breaks = scales::log_breaks()) +
  labs(title = "Modelo linearizado", caption = "Eixos na escala logarítmica")
```

-   $a = \exp(10,4) \approx 32.860,00;\, b \approx -0,25$

-   $PU = 32.860,00.Area^{-0,25}$

## Cobb-Douglas (6) {.smaller}

```{r}
library(ggpmisc)
ggplot(df, aes(x = Area/360, y = PU)) +
  geom_point() +
  stat_poly_line() +
  stat_poly_eq(use_label(c("eq", "R2")), label.x = "right", color = "blue",
               size = 7) +
  geom_segment(x = 0, xend = 0, y=0, yend = log(7560), color = "red", size=1.5) +
  geom_segment(x = 0, xend = -1, y=log(7560), x=log(7560), 
               color = "red", size=1.5) +
  scale_y_continuous(transform = "log") +
  scale_x_continuous(transform = "log", breaks = scales::log_breaks()) +
  labs(title = "Modelo Linearizado", 
       caption = "Eixos na escala logarítmica. Variável Área centralizada.")
```

-   $a = \exp(8,93) \approx 7.560,00;\, b \approx -0,25$

-   $PU = 7.560.\left (\frac{Area}{360} \right )^{-0,25} =  7.560.\left (\frac{360}{Area} \right )^{+0,25}$

# Dados de mercado

## Dados {.smaller}

-   Numa amostra de dados de mercado obtivemos:

-   30 dados de terrenos de variadas características

-   Algumas variáveis foram coletadas de forma incompleta, como a variável `Frente` e a variável `Incl` (inclinação da superfície do terreno).

. . .

```{r}
library(vtable)
st(dados, out = "kable", add.median = T)
```

-   Nota-se que existem apenas 25 dados com a variável `Frente`.
-   E que existem apenas 20 dados com a variável `Incl`.

## Modelo de Regressão Múltipla {.smaller}

### Apenas casos completos

```{r}
library(broom)
library(knitr)
library(kableExtra)
s <- summary(fit)
fit |>
  tidy(conf.int = T, conf.level = .80) |>
  kable(digits = 2, format.args = list(big.mark = ".", decimal.mark = ","),
        format = "html", 
        col.names = c("Termo", "Est.", "Erro", "Est. t", "p-valor",   
                      "Inf.", "Sup.")) |>
  footnote(alphabet = c(paste("Dados: ", length(fit$residuals)), 
                        paste("R2: ", brf(s$r.squared, digits = 2)),
                        paste("R2aj: ", brf(s$adj.r.squared, digits = 2))
                        ),
           escape = F)  |>
  add_header_above(c(" " = 5, "IC (80%)" = 2))
```

-   Variável `Frente` não se mostrou significante!

-   Por contar com apenas 15 dados completos, a estimação ficou prejudicada!

# Análise exploratória

## Variável Área

```{r}
p1 <- 
  ggplot(dados, aes(x = Area/450, y = PU)) +
  geom_point(color = "cornflowerblue", size = 2) +
  stat_smooth(method="nls", se=FALSE, formula=y~a*log(x/450) + k,
              method.args=list(start=c(a=-100, k=10000)))
p1
```

## Variável Frente

```{r}
library(mice)
library(ggmice)
p2 <-
  ggmice(dados, aes(x = Frente/15, y = PU)) +
  geom_point(size = 2) +
  stat_smooth(method="nls", se=FALSE, formula=y~a*log(x/15) + k,
              method.args=list(start=c(a=-100, k=10000)))  +
  theme_solarized(base_size = 15)
p2
```

## Variável Inclinação

```{r}
formula <- y ~ a*log1p(x) + k
p3 <- 
  ggmice(dados, aes(x = Incl, y = PU)) +
  geom_point(size = 2) +
  # stat_poly_line(formula = formula)
  stat_smooth(method="nls", se=FALSE, formula=y~a*log1p(x) + k,
              method.args=list(start=c(a=-100, k=10000)))  +
  theme_solarized(base_size = 15)
p3
```

# Derivação de Fatores

## Fator Área {.smaller}

```{r}
library(ggpmisc)
old.option <- options(OutDec = ",")
p1 +
  stat_poly_eq(use_label(c("eq", "R2", "F", "P"), sep = "*\"; \"*"), 
               formula = log(y) ~ log(x),
               coef.digits = 2, coef.keep.zeros = FALSE, 
               f.digits = 3, p.digits = 2,
               eq.with.lhs = "widehat(ln(PU))~`=`~",
               eq.x.rhs = ".ln(Area/450)",
               label.x = "left", label.y = "bottom", color = "red",
               size = 5)
```

-   Dado que a área tem correlação com a variável PU, na forma log-log, pode-se assim ajustar um fator área:

-   $$F_a = \left ( \frac{A_{imovel}}{A_{paradigma}} \right)^{-0,15} = \left ( \frac{A_{paradigma}}{A_{imovel}} \right)^{0,15} = \left ( \frac{450}{A_{imovel}} \right)^{0,15}$$

## Fator Área (2) {.smaller}

```{r}
#| echo: true
fitArea <- lm(log(PU) ~ log(Area/450), data = dados)
```

```{r}
fitArea |>
  tidy(conf.int = T, conf.level = .80) |>
  kable(digits = 2,
        col.names = c("Termo", "Est.", "Erro", "Est. t", "p-valor",                        "Inf.", "Sup.")) |>
  add_header_above(c(" " = 5, "IC (80%)" = 2))
```

-   De acordo com o modelo acima, um imóvel paradigma (`Area` = 450m2), tem VM de:

. . .

```{r}
#| echo: true
#predict(fitArea, newdata = list(Area = 450))
exp(7.81)
```

-   Já para um imóvel de 750m2, tem-se: $F_{Area} = (450/750)^{0,15} = 0,926$

-   Para avaliar o valor de mercado do lote de 750 m2: $E[PU|A=750] = 0,926.2.465=2282,60 \text{ R\$/m}^2$

. . .

```{r}
#| echo: true
#predict(fitArea, newdata = list(Area = 450))
p <- predict(fitArea, newdata = list(Area = 750))
exp(p)
```

## Fator Frente {.smaller}

```{r}
p2 +
  stat_poly_eq(use_label(c("eq", "R2", "F", "P"), sep = "*\"; \"*"), 
               formula = log(y) ~ log(x), 
               coef.digits = 2, coef.keep.zeros = FALSE, 
               f.digits = 2, p.digits = 2,
               eq.with.lhs = "widehat(ln(PU))~`=`~",
               eq.x.rhs = ".ln(Frente/15)",
               label.x = "left", label.y = "bottom", color = "red",
               size = 5)
```

-   Dado que não há evidência forte da correlação entre as variáveis `PU` e `Frente`, pode-se concluir que a variável `Frente` não é estatisticamente significante e, portanto, não é necessário o ajuste de um fator frente!

-   Além do mais, um fator frente assim ajustado seria contraditório: quanto maior a frente, menores os preços unitários!

## Fator inclinação {.smaller}

```{r}
p3 +
  stat_poly_eq(use_label(c("eq", "R2", "F", "P"), sep = "*\"; \"*"), 
               formula = log(y) ~ log1p(x), 
               coef.digits = 3, coef.keep.zeros = FALSE, 
               f.digits = 2, p.digits = 2,
               eq.with.lhs = "widehat(ln(PU))~`=`~",
               eq.x.rhs = ".ln(Incl+1)",
               label.x = "left", label.y = "bottom", color = "red",
               size = 5)
options(old.option)
```

-   Para a variável `Incl`, assim como para `Area`, há evidência de um efeito sobre `PU`.

-   Pode-se, assim, ajustar um fator inclinação:

-   $F_i = \left (\frac{i_{imovel} + 1}{i_{paradigma} + 1} \right )^{-0,10} = \left (\frac{i_{paradigma} + 1}{i_{imovel} + 1} \right )^{0,10} = \left (\frac{1}{i_{imovel} + 1} \right )^{0,10}$

## Fator inclinação (2) {.smaller}

```{r}
#| echo: true
fitIncl <- lm(log(PU) ~ log1p(Incl), data = dados)
```

```{r}
fitIncl |>
  tidy(conf.int = T, conf.level = .80) |>
  kable(digits = 2,
        col.names = c("Termo", "Est.", "Erro", "Est. t", "p-valor",                        "Inf.", "Sup.")) |>
  add_header_above(c(" " = 5, "IC (80%)" = 2))
```

1.  Para o lote paradigma (plano): $F_i = \left( \frac{1}{0+1}\right)^{0,10}=1,0$
2.  Para um lote com inclinação igual a 5%: $F_i = \left( \frac{1}{5+1}\right)^{0,10}=0,84$

-   O fator também deverá ser aplicado de forma ***multiplicativa***!
-   Se o lote paradigma (plano) possui valor igual a `r Reais(exp(coef(fitIncl)["(Intercept)"]))`/m2.
-   Então um lote com inclinação de 5% possui VM igual a `r Reais(.84*exp(coef(fitIncl)["(Intercept)"]))`/m2.

. . .

```{r}
#| echo: true
p <- predict(fitIncl, newdata = list(Incl = 5))
exp(p)
```

## Reflexões

-   No modelo de regressão linear múltipla, com menos dados disponíveis, o efeito da variável `Frente` era positivo, porém estatisticamente insignificante.

-   No modelo de regressão simples, com mais dados, o efeito da variável `Frente` também se mostrou insignificante, porém negativo.

-   Qual o efeito real da variável `Frente`?

## Efeito "real" da variável Frente {.smaller}

-   Análise da variável `Frente` *após* a homogeneização com o fator `Area`:

. . .

```{r}
dados <- within(dados, {
  FArea <- (450/Area)^.14
#  FIncl <- (1/(Incl + 1))^.10
  PUhom <- PU/FArea
})
```

```{r}
p2a <-
  ggmice(dados, aes(x = Frente/15, y = PUhom)) +
  geom_point(size = 2) +
  stat_poly_line() +
  # stat_smooth(method="nls", se=FALSE, formula=y~a*log(x/15) + k,
  #             method.args=list(start=c(a=-100, k=10000))) +
  theme_solarized(base_size = 15)
p2a +
  stat_poly_eq(use_label(c("eq", "R2", "F", "P"), sep = "*\"; \"*"), 
               formula = y ~ x, 
               coef.digits = 2, coef.keep.zeros = FALSE, 
               f.digits = 2, p.digits = 2,
               eq.with.lhs = "widehat(ln(PU[hom]))~`=`~",
               eq.x.rhs = ".ln(Frente/15)",
               label.x = "right", label.y = "bottom", color = "red",
               size = 5) +
  scale_y_continuous(transform = "log") +
  scale_x_continuous(transform = "log", breaks = scales::log_breaks()) +
  labs(caption = "Variável Frente centralizada.")
```

## Efeito "real" da variável Área {.smaller}

```{r}
dados <- within(dados, {
  FArea <- (450/Area)^.15
  FFrente <- (Frente/15)^.09
  PUhom2 <- PU/FFrente
})
```

```{r}
p1a <-
  ggmice(dados, aes(x = Area/450, y = PUhom2)) +
  geom_point(size = 2) +
  stat_poly_line() +
  # stat_smooth(method="nls", se=FALSE, formula=y~a*log(x/15) + k,
  #             method.args=list(start=c(a=-100, k=10000))) +
  theme_solarized(base_size = 15)
p1a +
  stat_poly_eq(use_label(c("eq", "R2", "F", "P"), sep = "*\"; \"*"), 
               formula = y ~ x, 
               coef.digits = 2, coef.keep.zeros = FALSE, 
               f.digits = 2, p.digits = 2,
               eq.with.lhs = "widehat(ln(PU[hom]))~`=`~",
               eq.x.rhs = ".ln(Area/450)",
               label.x = "right", label.y = "top", color = "red",
               size = 5) +
  scale_y_continuous(transform = "log") +
  scale_x_continuous(transform = "log", breaks = scales::log_breaks()) +
  labs(caption = "Variável Frente centralizada.")
```

-   $R^2$ passou de 0,28 para 0,34! Coeficiente passou de -0,14 para -0,16!

# Correlação Total, Parcial e Semiparcial

## Modelo com regressores Area e Frente {.smaller}

```{r}
fit1 <- lm(log(PU) ~ log(Area/450) + log(Frente/15), data = dados)
s1 <- summary(fit1)
fit1 |>
  tidy(conf.int = T, conf.level = .80) |>
  kable(digits = 2, format.args = list(big.mark = ".", decimal.mark = ","),
        format = "html", 
        col.names = c("Termo", "Est.", "Erro", "Est. t", "p-valor",
                      "Inf.", "Sup.")) |>
  footnote(alphabet = c(paste("Dados: ", length(fit1$residuals)),
                        paste("R2: ", 
                              brf(s1$r.squared, digits = 2)),
                        paste("R2aj: ", 
                              brf(s1$adj.r.squared, digits = 2))
                        ),
           escape = F)  |>
  add_header_above(c(" " = 5, "IC (80%)" = 2))
```

-   A retirada de `Incl` melhora a estimação dos outros coeficientes (mais dados)!

-   A variável `Frente`, agora, demonstrou-se significante e com efeito positivo.

-   O coeficiente de determinação diminui bastante com a retirada da variável `Incl`.

## Correlação de Ordem-Zero, Parcial e Semiparcial {.smaller}

-   Existem basicamente três tipos de correlação entre variáveis:

    -   A de Pearson (ordem-zero), quando analisadas isoladamente.

    -   A Parcial, computada enquanto se retira(m) o(s) efeito(s) de outra(s) variável(eis).

    -   A semi-parcial, que expressa a relação única entre uma variável independente e a variável dependente.

. . .

::::: columns
::: {.column width="50%"}
![Correlação Parcial](Partial.png)
:::

::: {.column width="50%"}
![Correlação Semi-parcial](SemiPartial.png)
:::
:::::

## Correlação de Ordem Zero, Parcial e Semi-Parcial (2) {.smaller}

```{r}
olsrr::ols_correlations(fit1) |>
  kable(digits = 2, format.args = list(decimal.mark = ",", big.mark = "."))
```

-   Na tabela acima são vistas a correlação de ordem zero, a parcial e a semi-parcial (coluna *Part*).

-   O valor da correlação semi-parcial elevado ao quadrado é também conhecido como **coeficiente de determinação parcial**!

    -   Por exemplo, para a variável `Frente`: $sr_{Frente}^2 = 0,42^2 \approx 0,18$.

    -   Já para a variável `Area`: $sr_{Area}^2 = -0,54^2 \approx 0,29$

-   O coeficiente de determinação parcial de uma variável representa o percentual de explicação que ela **adiciona** ao modelo!

    -   Por exemplo, a regressão simples da variável `Frente` vs. `PU` tinha $R^2 = 0,04$.
    -   Adicionando a variável `Area` a este modelo, ele ficou com $R^2 = 0,33$!

## Correlação de Ordem Zero, Parcial e Semi-Parcial (3) {.smaller}

```{r}
olsrr::ols_correlations(fit1) |>
  kable(digits = 2, format.args = list(decimal.mark = ",", big.mark = "."))
```

-   A correlação da variável `Area` com relação à PU era fraca ($r = -0,39$)
    -   Porém, após a consideração da variável `Frente`, essa correlação torna-se moderada ($pr = -0,55$)!
-   O mais importante, porém, é perceber que o sinal do coeficiente de correlação parcial muda para a variável `Frente`!
    -   A correlação da variável `Frente` com relação à `PU` era fraca e negativa ($r = -0,19$)
    -   Porém, na presença da variável `Area`, a correlação de `Frente` e `PU` passa a ser positiva e moderada ($pr = 0,45$)!
-   Este efeito, de mudança no sinal da correlação após a consideração de um outro regressor, é denominado de **Paradoxo de Simpson**!

# Paradoxo de Simpson

## Correlação entre Área e Frente {.smaller}

```{r}
ggmice(dados, aes(x = Frente/15, y = Area/450)) +
  geom_point() +
  stat_poly_line() +
  # stat_smooth(method="nls", se=FALSE, formula=y~a*log(x) + k,
  #             method.args=list(start=c(a=-100, k=10000))) +
  stat_poly_eq(use_label(c("eq", "R2", "F", "P")), 
               formula = y ~ x, 
               label.x = "left") +
  scale_y_continuous(transform = 'log', breaks = scales::log_breaks()) + 
  scale_x_continuous(transform = 'log') +
  theme_solarized(base_size = 15)
```

-   Existe uma forte correlação entre os regressores!

## Paradoxo de Simpson

-   $\ln(PU) = \beta_0 + \beta_1.\ln(Area/450) + \beta_2.\ln(Frente/15) + \varepsilon_1$

-   $\ln(Area) = \beta_3 + \beta_4.\ln(Frente/15) + \varepsilon_2$

-   $\ln(PU) = \beta_0 + \beta_1\beta_3 + (\beta_1\beta_4 + \beta_2).\ln(Frente/15) + \varepsilon$

-   $\ln(PU) = 7,81 - 0,33.\ln(Area/450) + 0,36\ln(Frente/15) + \varepsilon_1$

-   $\ln(Area/450) = -0,01 + 1,27.\ln(Frente) + \varepsilon_2$

-   $\hat{\ln(PU)} = 7,81 + (-0,33.1,27 + 0,36).\ln(Frente/15)$

-   $\hat{\ln(PU)} = 7,81 - 0,06.\ln(Frente/15)$

-   É por isso que a regressão simples com cada regressor não é, em geral, relevante para o ajuste de fatores de homogeneização!

## Paradoxo de Simpson (2)

```{r}
#| label: mcPlots
#| fig-cap: "Gráficos Marginais/Condicionais"
#| fig-width: 7
#| fig-height: 3.5

library(ggplotify)
library(ggeasy)
library(patchwork)
p1 <- as.ggplot(~mcPlot(fit1, "log(Area/450)", title = FALSE)) + 
  theme_solarized(base_size = 15) +
  easy_remove_axes()
p2 <- as.ggplot(~mcPlot(fit1, "log(Frente/15)", title = FALSE)) + 
  theme_solarized(base_size = 15) +
  easy_remove_axes()
p1 + p2
#p <- as.ggplot(function() (mcPlots(fit1, las = 1, title = FALSE)))
# p + theme_solarized()
```

-   Nos modelos de regressão linear múltipla, o efeito de uma variável é computado após a "homogeneização" da outra!

## Resíduos Parciais

```{r}
plotModel(fit1, residuals = T)
```

## Derivação de Fatores a partir da RLM {.smaller}

```{r}
fit1 |>
  tidy(conf.int = T, conf.level = .80) |>
  kable(digits = 2, format.args = list(big.mark = ".", decimal.mark = ","),
        format = "html", 
        col.names = c("Termo", "Est.", "Erro", "Est. t", "p-valor",
                      "Inf.", "Sup.")) |>
  add_header_above(c(" " = 5, "IC (80%)" = 2))
```

-   Os fatores derivados devem ser utilizados na forma ***multiplicativa***!

    -   A equação de estimação será: $PU = 2.465,00.F_{Area}.F_{Frente}$

-   O fator Área seria: $F_{Area} = \left ( \frac{450}{A_{imovel}} \right)^{0,33}$

-   O fator Frente seria: $F_{Frente} = \left ( \frac{F_{imovel}}{15} \right)^{0,36}$

## Aplicação do Fator `Incl` {.smaller}

-   Como vimos, é possível ajustarmos bons modelos com a remoção de uma variável que conta com muitos valores faltantes, como a variável `Incl`.

-   No entanto, se a sua remoção do modelo melhora a estimação, por outro lado, o efeito desta variável não pode ser simplesmente ignorado!

-   Uma alternativa é derivar o fator `Incl` a partir de uma regressão simples:

-   Por exemplo, imagine a avaliação de um lote com `Area` = 750m, `Frente` = 25m, porém com 10% de inclinação.

    -   Pode-se aplicar o fator `Incl`: $F_i = \left (\frac{1}{i_{imovel} + 1} \right )^{0,10}$
    -   Assim, o valor de mercado de um lote com `Area` = 750m, `Frente` = 25m e `Incl` igual a 10% é igual a:
    -   $PU = 2.465,00.\left ( \frac{450}{A_{imovel}} \right)^{0,33}.\left ( \frac{F_{imovel}}{15} \right)^{0,36}.\left (\frac{1}{i_{imovel} + 1} \right )^{0,10}$
    -   $PU = 2.465,00.\left ( \frac{450}{750} \right)^{0,33}.\left ( \frac{25}{15} \right)^{0,36}.\left (\frac{1}{10 + 1} \right )^{0,10} \approx$`r Reais(exp(coef(fit1)[1])*.845*1.2*.79)`

# Reflexões

## Análise Exploratória

```{r}
library(car)
library(ggplotify)

p <- as.ggplot(function() (
scatterplotMatrix(~ log(PU) + log(Area) + log(Frente) + log(Incl), 
                  data = dados, smooth = FALSE, las = 1)))
p + theme_solarized() +
  easy_remove_axes()
  # theme(axis.text.x = element_blank(),
  #       axis.ticks.x = element_blank(), 
  #       axis.text.y = element_blank(), 
  #       axis.ticks.y = element_blank(),
  #       axis.title.x = element_blank(),
  #       axis.title.y = element_blank()) 
```

## Relação de Inclinação com outros regressores {.smaller}

```{r}
#| label: fig-InclinacaoOthers
#| fig-cap: "Relação da variável Inclinação com outras VE."
#| fig-subcap:
#|   - "Incl vs. Area"
#|   - "Incl vs. Frente"
#| layout-ncol: 2
ggmice(dados, aes(x = Incl, y = Area)) +
  geom_point(size = 2) +
  stat_smooth(method="nls", se=FALSE, formula=y~a*log(x/360) + k,
              method.args=list(start=c(a=-100, k=10000))) +
  stat_poly_eq(use_label(c("eq", "R2", "F", "P")), 
               formula = log(y) ~ log(x), 
               label.x = "right", color = "red") +
  theme_solarized(base_size = 15)
ggmice(dados, aes(x = Incl, y = Frente)) +
  geom_point(size = 2) +
  stat_smooth(method="nls", se=FALSE, formula=y~a*log(x/360) + k,
              method.args=list(start=c(a=-100, k=10000))) +
  stat_poly_eq(use_label(c("eq", "R2", "F", "P")), 
               formula = log(y) ~ log(x), 
               label.x = "right", color = "red") +
  theme_solarized(base_size = 15)
```

-   Não há, na prática, qualquer relação entre `Incl` e as variáveis `Frente` e `Area`.
    -   Isto permite que o fator `Incl`, derivado a partir de uma regressão simples, seja utilizado sem qualquer preocupação a respeito de viés de variável omitida.

## Reflexões {.smaller}

-   No mercado imobilário existem diversas, inúmeras variáveis explicativas

-   Muitas delas estão correlacionadas entre si, a exemplo das variáveis `Area` e `Frente`

    -   Quando as variáveis explicativas estão correlacionadas, não se pode ignorar o efeito da presença/ausência de uma sobre a outra na modelagem

-   Outras variáveis não estão necessariamente correlacionadas com as outras. É o exemplo da variável `Incl`. Podemos também citar a variável `Andar`, em apartamentos, ou a posição no andar

    -   Quando as variáveis não estão potencialmente correlacionadas com as outras variáveis do modelo, faz sentido a derivação de um fator de maneira isolada.
    -   Quando a variável em análise estiver potencialmente correlacionada apenas com algumas das variáveis, pode ser necessário um modelo de regressão múltipla auxiliar com o objetivo de estimar, sem viés, o seu coeficiente.

# Aparte: Imputação de dados

## Aparte: imputação de dados {.smaller}

-   Existem algoritmos de imputação de múltipla de dados, como o PMM [@rubin1986], que permitem a imputação de dados faltantes mesmo na presença de não-linearidade ou heteroscedasticidade.

. . .

```{r, include = FALSE}
d <- mice(dados, method = "pmm", m = 1, seed = 20,
     formulas = list("Frente" = log(Frente) ~ log(PU) + log(Area) +
                       log(Incl),
                     "Incl" = log(Incl) ~ log(Frente) + log(PU) +
                       log(Area))
)
```

```{r}
nlsFit <- nls(Area~a*log(Frente) + k, data = dados, start = c(a=-100, k=10000))
FUN <- function(x) coef(nlsFit)["a"]*log(x) + coef(nlsFit)["k"]
ggmice(d, aes(x = Frente, y = Area)) +
  geom_point(size = 2) +
  geom_function(fun = FUN, color = "cornflowerblue", size = 1) +
  theme_solarized(base_size = 15) +
  labs(caption = "Dados imputados através de Correspondência de Média Preditiva.")
```

## Aparte: imputação de dados (2)

```{r}
nlsFit <- nls(PU~a*log(Incl) + k, data = dados, start = c(a=-100, k=10000))
FUN <- function(x) coef(nlsFit)["a"]*log(x) + coef(nlsFit)["k"]
ggmice(d, aes(x = Incl, y = PU)) +
  geom_point(size = 2) +
  geom_function(fun = FUN, color = "cornflowerblue", size = 1) +
  theme_solarized(base_size = 15) +
  labs(caption = "Dados imputados através de Correspondência de Média Preditiva.")
```

## Aparte: imputação de dados (3) {.smaller}

### Modelo com dados imputados

```{r}
ImpFit <- lm(log(PU) ~ log(Area/450) + log(Frente/15) + log1p(Incl), 
             data = complete(d))
sImp <- summary(ImpFit)
ImpFit |>
  tidy(conf.int = T, conf.level = .80) |>
  kable(digits = 2, format.args = list(big.mark = ".", decimal.mark = ","),
        format = "html",
        col.names = c("Termo", "Est.", "Erro", "Est. t", "p-valor", 
                      "Inf.", "Sup.")) |>
  footnote(alphabet = c(paste("Dados: ", length(ImpFit$residuals)),
                        paste("R2: ", 
                              brf(sImp$r.squared, digits = 2)),
                        paste("R2aj: ", 
                              brf(sImp$adj.r.squared, digits = 2))
                        ),
           escape = F)  |>
  add_header_above(c(" " = 5, "IC (80%)" = 2))
```

-   A imputação de dados permite o ajuste de modelos com todas as variáveis, aproveitando todas as informações disponíveis.

-   Este modelo pode ser utilizado, posteriormente, para o ajuste de fatores.

## Aparte: imputação de dados (4) {.smaller}

-   O procedimento de imputação de dados poderia ser padronizado pela NBR 14.653, visando permitir imputações de dados, desde que baseados em procedimentos pré-definidos!

-   @doi:10.1080/02664763.2016.1158246: Melhor método para imputação de dados em pequenas amostras: *joint multiple imputation* (JMI)!

. . .

```{r}
library(JointAI)
dados <- within(dados,{
  logPU <- log(PU)
  logArea <- log(Area/450)
  logFrente <- log(Frente/15)
  logIncl <- log1p(Incl)
})
ImpFit1 <- lm_imp(logPU ~ logArea + logFrente + logIncl, data = dados, 
                  n.iter = 1000, seed = 1)
summary(ImpFit1)
```

# Fatores Aditivos

## Introdução {.smaller}

-   Os fatores multiplicativos derivam de um modelo multiplicativo

    -   $PU = \beta_0.\exp(\beta_1)^{X_1}\ldots\exp(\beta_k)^{X_k}.\varepsilon$

-   Da mesma maneira, os fatores aditivos, devem ser derivados de um modelo aditivo

    -   $PU = \beta_0+\beta_1X_1 + \ldots + \beta_kX_k + \varepsilon$

-   Apesar do erro dificilmente aparecer no mercado imobiliário na forma aditiva

-   Existem técnicas que permitem o ajustamento de modelos com erros não-constantes

    -   MQP: @wls2024
    -   Ressuscitando MQP: @ROMANO20171

-   Trabalhar na escala original (PU) tem algumas vantagens

    -   Ausência de distorções devido às transformações
    -   Desnecessidade de retransformação da variável

## Derivação de Fatores Aditivos {.smaller}

-   Como bem observou @lima2006, se na avaliação por fatores aditivos, temos:

    -   $$\hat{PU_i} = \bar{PU}_{hom}.[1 + (F_{1i}-1) + (F_{2i}-1) + \ldots + (F_{ki}-1)]$$

-   E na avaliação por regressão múltipla, temos:

    -   $$\hat{PU_i} = \hat{\beta_0}.+ \hat{\beta_1}X_{1i} + \hat{\beta_2}X_{2i} + \ldots + \hat{\beta_k}X_{ki}$$

-   $$\therefore \left\{\begin{matrix}
    \hat\beta_0 = \bar{PU}_{hom} \\
    F_{1i}=\hat{\beta_1}/\hat{\beta_0}.X_{1i} \\
    F_{2i}=\hat{\beta_2}/\hat{\beta_0}.X_{2i}  \\
    \cdots \\
    F_{ki}=\hat{\beta_k}/\hat{\beta_0}.X_{ki}
    \end{matrix}\right.$$

-   Porém, quem disse que $\hat\beta_0 = \bar{PU}_{hom}$?

    -   $\hat\beta_0 = \bar{PU}_{hom}\Leftrightarrow$ as variáveis explicativas estiverem centralizadas!

## Modelo aditivo minimalista {.smaller}

```{r}
library(readxl)
veyron <- read_excel("data/veyron.xlsx")
veyron <- within(veyron, {
  Ano <- factor(Ano)
  PC <- factor(PC, levels = c("Baixo", "Medio", "Alto"))
  Infra <- factor(Infra, ordered = T)
  Studio <- factor(Studio)
  RendaMedia <- RendaMedia/1212 # transforma renda média para s.m.
})
```

```{r}
fit <- lm(PU ~ PC + Ano + I(Idade-10) + log(AP/100), data = veyron)
s <- summary(fit)
fit |>
  tidy(exponentiate = F, conf.int = T, conf.level = .80) |>
  kable(digits = 2, 
        format.args = list(big.mark = ".", decimal.mark = ","),
        format = "html", 
        col.names = c("Termo", "Est.", "Erro", "Est. t", "p-valor",           
                      "Inf.", "Sup.")) |>
  footnote(alphabet = c(paste("Dados: ", length(fit$residuals)), 
                        paste("R2: ", brf(s$r.squared, digits = 2)),
                        paste("R2aj: ", brf(s$adj.r.squared, digits = 2))
                        ),
           escape = F)  |>
  add_header_above(c(" " = 5, "IC (80%)" = 2))
```

-   Como as variáveis explicativas estão centralizadas no imóvel paradigma, então $\hat\beta_0 = \bar{PU}_{hom}$!

## Interpretação das variáveis {.smaller}

-   Para uma derivação formal dos fatores, podem ser utilizadas as derivadas parciais da equação de regressão em relação a cada termo.

-   Para a variável `Ano`:

    -   $$\frac{\partial PU(PC, Ano, Idade, AP)}{\partial \text{Ano}} = \frac{\partial  3.623,05.\text{Ano}}{\partial \text{Ano}} = 3.623,05$$

    -   $\therefore \delta PU = 3.623,05.\delta Ano$

-   Para a variável `Idade`: $$\frac{\partial PU(PC, Ano, Idade, AP)}{\partial \text{Idade}} = \frac{\partial -116,45.(\text{Idade-10})}{\partial \text{Idade}}=-116,45$$

    -   $\therefore \delta PU = -116,45.\delta Idade$

    -   Ou seja, se a Idade do avaliando for 5 anos ($\delta Idade = -5$), então $F_{Idade} = -116,45.(-5) = +582,25$

## Interpretação das variáveis (2) {.smaller}

-   Para a variável `Area`:

    -   $$\frac{\partial PU(PC, Ano, Idade, AP)}{\partial AP} = -1.049,06 \frac{\partial log(AP/100)}{\partial AP}$$

    -   $$\frac{\partial PU}{\partial AP} = -1.049,06\frac{1}{AP} \Leftrightarrow \delta PU = -1.049,06 \frac{\delta AP}{AP}$$

    -   O aumento de 1% em `AP` corresponde a uma diminuição de -R\$ 10,49 em PU!

    -   Cuidado: a interpretação acima vale para pequenos percentuais de variação!

-   O fator área deve ser calculado: $F_{Area} = -1.049,06.\ln(Area/100)$

    -   Por exemplo: para um imóvel com área igual a 272 $m^2$:
    -   $F_{Area}=-1.049,06.\ln(272/100) = -1.049,06 \text{ R\$}/m^2$

-   Não é possível utilizar fatores do tipo: $\left(\frac{X_{imóvel}}{X_{paradigma}}\right)^\alpha$ de forma aditiva

## Derivação de Fatores Aditivos {.smaller}

### Com o método de @lima2006 ajustado

-   Fator Padrão Construtivo:
    -   $$F_{PC} =  \begin{cases}
          6.147,07/6.147,07 = 1,00 & \text{ se } PC=Baixo \\
          (6.147,07+2.048,54)/6.147,07 = 1,33 & \text{ se } PC=Medio \\
          (6.147,07+5.151,55)/6.147,07 = 1,84 & \text{ se } PC=Alto 
          \end{cases}$$
-   Fator Idade:
    -   $$F_{Idade} = -0,019.(Idade-10) + 1$$
-   Fator Ano:
    -   $$F_{Ano} =  \begin{cases}
        6.147,07/6.147,07 = 1,00 & \text{ se } Ano = 2018\\
        (6.147,07+3.623,05)/6.147,07 = 1,59 & \text{ se } Ano = 2022
        \end{cases}$$

## Derivação de Fatores Aditivos {.smaller}

### Com o método de @lima2006 ajustado

-   Fator AP:

    -   $$F_{AP} = \frac{-1.049,06}{6.147,07}.\ln(AP_{imovel}/AP_{paradigma})+1=-0,17.\ln(AP_{imovel}/100)+1$$

-   Aplicação: Se o imóvel paradigma (100m2, ano 2018, 10 anos de idade, PC Baixo) tem valor unitário de mercado igual a R\$ 6.147,07/m2

-   Quanto vale um apartamento de 200m2, 5 anos de idade, PC Médio, no ano de 2022?

    -   $PU_i=6.147,07[1+[(1,33-1)+(1,095-1)+(1,59-1)+(0,88-1)]$
    -   $PU_i=6.147,07[1+(0,33+0,095+0,59-0,12)]$
    -   $PU_i=6.147,07.1,895$
    -   $PU_i=11.648,70$

## Comparação com previsões do modelo {.smaller}

```{r}
#| echo: true
predict(fit, newdata = list(PC = "Medio", Ano = "_2022", Idade = 5, AP = 200))
```

-   O método de @lima2006 funciona!
    -   Porém, deve-se perceber: só funcionou porque o modelo foi ajustado centralizado no imóvel paradigma.
    -   Assim, o valor de $\beta_0$ se igualou ao valor do imóvel paradigma e permitiu a estimação consistente dos fatores de homogeneização!

# Conclusão

```{r, eval = FALSE}
library(readxl)

df <- read_excel("data/dados.xlsx")

colnames(df) <- c("Id", "Endereço", "Complemento", "Area", "PercentualTerreno",
                  "Distancia", "Garagem", "Idade", "PU")


df <- within(df, {
  Garagem <- ifelse(Garagem == 1, " Sim", " Não")
  # Idade <- ifelse(Idade == 1, " <= 10 anos", " > 10 anos")
  Idade <- factor(Idade, levels = c(0, 1), labels = c(" > 10 anos", " <= 10 anos"))
  ATerreo <- Area*PercentualTerreno/100
  ASuperiores <- Area*(1-PercentualTerreno/100)
  Relacao <- ASuperiores/ATerreo
})

# write.table(df, file = "EC/dados.tsv", row.names=FALSE, sep="\t")


fit <- lm(log(PU) ~ log(Area/1500) + log(PercentualTerreno/25) +
            log(Distancia/100) + Garagem + Idade, data = df)
summary(fit)
```

## Conclusão {.smaller}

-   Este é um modelo ruim, além de mal comunicado:
    -   $VU = -566,45-3,0555.10^{-9}.A^3 +548,17.\text{SetorUrbano}^{0,5}+143,27.\ln(Frente(\text{qualitativa}))$
-   Este é um bom modelo, porém ainda mal comunicado:
    -   $\hat{\ln(PU)} = 4,768-0,2703.\ln(A) + 0,2339.\ln(\%T)-0,0983.\ln(D)+0,3735.G+0,5186.I + \varepsilon$
-   Este é um bom modelo, porém melhor comunicado:
    -   $\hat{PU} = 117,68.A^{-0,270}.(\%T)^{0,234}.D^{-0,098}.1,45^{G}.1,68^I$
-   A centralização dos dados melhora a comunicação:
    -   $\hat{PU} = 3,10.(A/1500)^{-0,270}.(\%T/25\%)^{0,234}.(D-100)^{-0,098}.1,45^{G}.1,68^I$
-   Argumento que o modelo poderia ser assim exposto, para o melhor entendimento 
do cliente (leigo):
    -   $\hat{PU} = 3,10.F_{Area}.F_{\%Terreo}.F_{Distância}.F_{Garagem}.F_{Idade}$

## Conclusão {.smaller}

-   Os fatores de homogeneização continuam úteis!

-   Com os novos estudos, agora estão se tornando também, consistentes!

-   A centralização de variáveis é importante para garantir uma estimação consistente dos fatores de homogeneização

-   A independência ou correlação entre os regressores é importante para fins de definir se um fator pode ser ajustado através de uma regressão simples

-   A clareza do Tratamento por Fatores deve ser trazida para o Tratamento Científico

-   E a consistência do Tratamento Científico deve ser levada para o Tratamento por Fatores!

## Referências
